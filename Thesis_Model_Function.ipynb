{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f04db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "from functools import reduce\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates the person days for each month. ie exposure - population times number of days in the month\n",
    "def person_days(df):\n",
    "    result = df.copy()\n",
    "    for col in df.columns:\n",
    "        mon = col[-2:]\n",
    "        \n",
    "        # don't multiply column which are relevant data \n",
    "        if len(col) < 5:\n",
    "            result[col] = df[col]\n",
    "            #print(\"Col \" + col + \" not multiplied\")\n",
    "            \n",
    "            # September, April, June and November by 30\n",
    "        elif mon in ['09','04','06','11']:\n",
    "            result[col] = df[col]*30\n",
    "            #print(\"Col \" + col + \" Multiplied by 30\")\n",
    "        elif mon in ['01','03','05','07','08','10','12']:\n",
    "            result[col] = df[col]*31\n",
    "            #print(\"Col \" + col + \" Multiplied by 31\")\n",
    "            \n",
    "            # February leap years by 29\n",
    "        elif mon == '02' and col[-5:-3] in ['16','20']:\n",
    "            result[col] = df[col]*29\n",
    "            #print(\"Col \" + col + \" Multiplied by 29\")       \n",
    "            \n",
    "            # non-leap year february by 28\n",
    "        elif mon == '02' and col[-5:-3] not in ['16','20']:\n",
    "            result[col] = df[col]*28\n",
    "            #print(\"Col \" + col + \" Multiplied by 28\")\n",
    "    return result\n",
    "\n",
    "# undoes the previous function - divides by number of days in each month\n",
    "def undo_person_days(df):\n",
    "    result = df.copy()\n",
    "    for col in df.columns:\n",
    "        mon = col[-2:]\n",
    "        \n",
    "        # don't multiply column which are relevant data \n",
    "        if len(col) < 5:\n",
    "            result[col] = df[col]\n",
    "            #print(\"Col \" + col + \" not multiplied\")\n",
    "            \n",
    "            # September, April, June and November by 30\n",
    "        elif mon in ['09','04','06','11']:\n",
    "            result[col] = df[col]/30\n",
    "            #print(\"Col \" + col + \" Multiplied by 30\")\n",
    "        elif mon in ['01','03','05','07','08','10','12']:\n",
    "            result[col] = df[col]/31\n",
    "            #print(\"Col \" + col + \" Multiplied by 31\")\n",
    "            \n",
    "            # February leap years by 29\n",
    "        elif mon == '02' and col[-5:-3] in ['16','20']:\n",
    "            result[col] = df[col]/29\n",
    "            #print(\"Col \" + col + \" Multiplied by 29\")       \n",
    "            \n",
    "            # non-leap year february by 28\n",
    "        elif mon == '02' and col[-5:-3] not in ['16','20']:\n",
    "            result[col] = df[col]/28\n",
    "            #print(\"Col \" + col + \" Multiplied by 28\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e811ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(k_t,a_x, b_x,D, pop, deaths,terms): # input K_t is a flat nx1 vector \n",
    "    k_t = k_t.reshape((terms,pop.shape[1]))\n",
    "    exp_term = np.multiply(np.exp(a_x + b_x@np.diag(D)[:terms,:terms]@k_t),pop)\n",
    "    return np.sum((deaths - exp_term)**2)\n",
    "\n",
    "def regional_objective(k_t,b_x,K_t,B_x,a_xi,D_country,D,region_pop,region_death,regional_terms):\n",
    "    k_t = k_t.reshape((regional_terms,region_pop.shape[1]))\n",
    "    exp_term = np.multiply(np.exp(a_xi + B_x@np.diag(D_country)[:K_t.shape[0],:K_t.shape[0]]@K_t + b_x@np.diag(D)[:regional_terms,:regional_terms]@k_t),region_pop)\n",
    "    return np.sum((region_death - exp_term)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_exogenous(exog_table,numlags=12):\n",
    "    factor_names = exog_table.columns\n",
    "    numlags = 12\n",
    "    for factor in factor_names:\n",
    "        for l in range(numlags):\n",
    "            name = factor + \"_lag\" + str(l+1)\n",
    "            exog_table[name] = exog_table[factor].shift(l+1).bfill()\n",
    "    return exog_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_smoothing(df, window_size=3):\n",
    "    # Function to apply rolling mean\n",
    "    def apply_rolling(series):\n",
    "        return series.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "\n",
    "    # Apply the smoothing function to each row of the DataFrame\n",
    "    smoothed_df = df.apply(apply_rolling, axis=1)\n",
    "    return smoothed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39b35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_adjust(df, apply_moving_average=False, window_size=12):\n",
    "    \"\"\"\n",
    "    Adjust the population data for all age groups using proportionate allocation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe containing population data with age groups as rows and months as columns.\n",
    "    apply_moving_average (bool): Whether to apply moving average smoothing. Default is False.\n",
    "    window_size (int): The window size for moving average. Default is 12.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The adjusted population data for all age groups.\n",
    "    \"\"\"\n",
    "    adjusted_df = df.copy()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Extract the specified age group data\n",
    "        age_group_data = row.values\n",
    "\n",
    "        # Initialize adjusted data for the specified age group\n",
    "        adjusted_data = age_group_data.copy()\n",
    "\n",
    "        # Calculate the yearly changes and distribute proportionally\n",
    "        for i in range(1, len(age_group_data)//12):\n",
    "            start_index = i * 12\n",
    "            end_index = start_index + 12\n",
    "            change = age_group_data[start_index] - age_group_data[start_index - 1]\n",
    "            proportional_change = change / 12\n",
    "\n",
    "            for j in range(12):\n",
    "                adjusted_data[start_index + j] -= proportional_change * (j + 1)\n",
    "\n",
    "        # Apply moving average if requested\n",
    "        if apply_moving_average:\n",
    "            adjusted_data = np.convolve(adjusted_data, np.ones(window_size)/window_size, mode='valid')\n",
    "            # Adjust the length of the data to match the original\n",
    "            adjusted_data = np.concatenate((adjusted_data, np.full(len(age_group_data) - len(adjusted_data), np.nan)))\n",
    "        \n",
    "        # Update the dataframe with the adjusted data\n",
    "        adjusted_df.loc[index, :] = adjusted_data\n",
    "    \n",
    "    return adjusted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d27bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(country_data,regional_data,\n",
    "               common_terms = 1,regional_terms = 1,\n",
    "               prediction_period = 36,train_period = 84,\n",
    "               exogenous_train = None,exogenous_test = None,\n",
    "              country_deaths = None, country_pop = None,\n",
    "              region_deaths = None, region_pop = None,re_est = False,\n",
    "              jump_off = False,\n",
    "              smooth = False):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import pmdarima as pm\n",
    "    from functools import reduce\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    if smooth:\n",
    "        regional_data = ma_smoothing(regional_data)\n",
    "        country_data = ma_smoothing(country_data)\n",
    "    \n",
    "    regional_logmort = regional_data.iloc[:,:train_period]\n",
    "    country_logmort = country_data.iloc[:,:train_period]\n",
    "    regional_test = regional_data.iloc[:,train_period:]\n",
    "    country_test = country_data.iloc[:,train_period:]\n",
    "        \n",
    "\n",
    "    if country_data.shape != regional_data.shape:\n",
    "        return \" Error Country and Regional table shapes do not match\"\n",
    "\n",
    "     ### LC for country level \n",
    "    a_x = country_logmort.mean(axis = 1) #country_logmort.mean(axis = 1) # average over time for each age group\n",
    "    A_xt =  country_logmort.sub(a_x,axis = 0) #logmort - a_x\n",
    "    a_x = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D_country,V_t = np.linalg.svd(A_xt,full_matrices = False) # SVD of A_xt (should it be only logmort as in Lee-Li model?)\n",
    "\n",
    "    B_xs = np.zeros((country_logmort.shape[0],common_terms)) # columns are B_x terms\n",
    "    K_ts = np.zeros((common_terms,country_logmort.shape[1])) # rows are K_t terms \n",
    "    country_re_est = np.zeros(country_logmort.shape)\n",
    "    for i in range(common_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        B_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        p_value = sm.stats.acorr_ljungbox(k_t, lags=[len(k_t) - 1],return_df = True).iloc[0,1]\n",
    "        if p_value > 0.05:\n",
    "            print(\"Common k_t\"+str(i)+ \" is white noise with LB p_value \"+ str(p_value))\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(1,len(k_t)))\n",
    "        K_ts[i,:] = k_t\n",
    "        country_re_est = country_re_est + b_x@k_t\n",
    "        country_re_est = B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_ts\n",
    "            \n",
    "        \n",
    "\n",
    "        ### LC for regional level\n",
    "    regional_logmort_adj = regional_logmort - country_re_est\n",
    "    a_xi = regional_logmort_adj.mean(axis = 1) # average over time for each age group\n",
    "    if jump_off:\n",
    "        a_xi = np.array(regional_data.iloc[:,train_period])\n",
    "        a_xi = a_xi.reshape(regional_data.shape[0],1) \n",
    "    else:\n",
    "        a_xi = np.reshape(a_xi, (len(a_xi), 1))\n",
    "    \n",
    "    A_xt_reg = regional_logmort_adj.sub(a_xi,axis = 1) #logmort - a_x\n",
    "    #a_xi = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D,V_t = np.linalg.svd(A_xt_reg,full_matrices = False) # SVD of A_xt\n",
    "\n",
    "    b_xs = np.zeros((regional_logmort.shape[0],regional_terms)) # columns are b_x terms\n",
    "    k_ts = np.zeros((regional_terms,regional_logmort.shape[1])) # rows are k_t terms\n",
    "    #regional_re_est = np.zeros(regional_logmort.shape)\n",
    "    for i in range(regional_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        b_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        p_value = sm.stats.acorr_ljungbox(k_t, lags=[len(k_t) - 1],return_df = True).iloc[0,1]\n",
    "        print(\"p_value k_t\" +str(i)+\" before re-estimation: \" +str(p_value)+\" \\n\")\n",
    "        if p_value > 0.05:\n",
    "            print(\"Regional k_t\"+str(i)+ \" is white noise with LB p_value \"+ str(p_value)+ \" \\n\")\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(1,len(k_t)))\n",
    "        k_ts[i,:] = k_t\n",
    "            \n",
    "        #regional_re_est = regional_re_est + b_x@k_t\n",
    "   \n",
    "    # re-estimation of k_t terms\n",
    "    res = 0\n",
    "    if re_est:\n",
    "        K_ts_reest = K_ts.flatten() #flatten as minimize only takes 1d array of parameters\n",
    "        #print(objective(K_ts_reest,np.zeros((country_pop.shape[0],1)), B_xs,D_country, country_pop, country_deaths,common_terms))\n",
    "        result = minimize(objective, K_ts_reest, args=(a_xi, B_xs,D_country, country_pop, country_deaths,common_terms), method='BFGS') #np.zeros((country_pop.shape[0],1))\n",
    "        #print(objective(result.x,np.zeros((country_pop.shape[0],1)), B_xs,D_country, country_pop, country_deaths,common_terms))\n",
    "        K_ts = result.x.reshape(((common_terms,country_pop.shape[1])))\n",
    "        \n",
    "        k_ts_reest = k_ts.flatten()\n",
    "        print(\"Objecive before minimisation \" +str(regional_objective(k_ts_reest,b_xs,K_ts,B_xs,a_xi,D_country,D,region_pop,region_deaths,regional_terms)))\n",
    "        result = minimize(regional_objective,k_ts_reest,args = (b_xs,K_ts,B_xs,a_xi,D_country,D,region_pop,region_deaths,regional_terms),method = 'BFGS')\n",
    "        print(\"Objective after minimisation \" + str(regional_objective(result.x,b_xs,K_ts,B_xs,a_xi,D_country,D,region_pop,region_deaths,regional_terms)))\n",
    "        k_ts = result.x.reshape(((regional_terms,region_pop.shape[1])))\n",
    "        \n",
    "    #regional_re_est = regional_re_est + a_xi\n",
    "    regional_re_est = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_ts + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts \n",
    "    #a_xi_adjustment = np.subtract(np.array(regional_logmort),regional_re_est)\n",
    "    #a_xi = a_xi + np.mean(a_xi_adjustment,axis = 1).transpose()\n",
    "    #regional_re_est = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_ts + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts\n",
    "    a_xi = np.reshape(np.mean(np.subtract(np.array(regional_logmort),regional_re_est),axis = 1),(regional_logmort.shape[0],1)) +a_xi\n",
    "    regional_re_est = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_ts + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts\n",
    "\n",
    "        # model country terms    \n",
    "    K_t_models = {}\n",
    "    K_t_preds = np.zeros((common_terms,train_period + prediction_period))\n",
    "    K_t_preds_upconf = np.zeros((common_terms,train_period + prediction_period))\n",
    "    K_t_preds_lowconf = np.zeros((common_terms,train_period + prediction_period)) #country_logmort.shape[0]\n",
    "    for i in range(common_terms):\n",
    "        K_t_model = pm.auto_arima(K_ts[i,:].flatten(),\n",
    "                               start_p = 0,\n",
    "                               start_q = 0,\n",
    "                               max_p = 15,\n",
    "                               max_q = 15,\n",
    "                               max_d = 4,\n",
    "                               m = 12,\n",
    "                               seasonal = True,\n",
    "                               suppress_warnings=True,\n",
    "                               error_action='ignore',with_intercept = True)\n",
    "\n",
    "        K_t_models[f'model_{i}'] = K_t_model\n",
    "        K_t_model_preds,K_t_model_confint = K_t_model.predict(n_periods = prediction_period,return_conf_int = True)\n",
    "        K_t_preds[i,:] = np.append(K_ts[i,:],np.array(K_t_model_preds).reshape((1,len(K_t_model_preds))))\n",
    "        K_t_preds_upconf[i,:] = np.append(K_ts[i,:],np.array(K_t_model_confint[:,0]).reshape((1,len(K_t_model_confint[:,0]))))\n",
    "        K_t_preds_lowconf[i,:] = np.append(K_ts[i,:],np.array(K_t_model_confint[:,1]).reshape((1,len(K_t_model_confint[:,1]))))\n",
    "    \n",
    "\n",
    "        # models for regional terms \n",
    "    k_t_models = {}\n",
    "    k_t_preds = np.zeros((regional_terms,train_period + prediction_period))\n",
    "    k_t_preds_upconf = np.zeros((regional_terms,train_period + prediction_period))\n",
    "    k_t_preds_lowconf = np.zeros((regional_terms,train_period + prediction_period)) #regional_logmort.shape[0]\n",
    "    for i in range(regional_terms):\n",
    "        k_t_model = pm.auto_arima(k_ts[i,:].flatten(),\n",
    "                               exogenous_train,\n",
    "                               start_p = 0,\n",
    "                               start_q = 0,\n",
    "                               max_p = 15,\n",
    "                               max_q = 15,\n",
    "                               max_d = 4,\n",
    "                               m = 12,\n",
    "                               seasonal = True,\n",
    "                               suppress_warnings=True,\n",
    "                               error_action='ignore',with_intercept = True)\n",
    "\n",
    "        k_t_models[f'model_{i}'] = k_t_model\n",
    "        k_t_model_preds,k_t_model_confint = k_t_model.predict(n_periods = prediction_period,X = exogenous_test,return_conf_int = True)\n",
    "        k_t_preds[i,:] = np.append(k_ts[i,:],np.array(k_t_model_preds).reshape((1,len(k_t_model_preds))))\n",
    "        k_t_preds_upconf[i,:] = np.append(k_ts[i,:],np.array(k_t_model_confint[:,0]).reshape((1,len(k_t_model_confint[:,0]))))\n",
    "        k_t_preds_lowconf[i,:] = np.append(k_ts[i,:],np.array(k_t_model_confint[:,1]).reshape((1,len(k_t_model_confint[:,1]))))\n",
    "            \n",
    "            \n",
    "    if jump_off:\n",
    "        a_xi = np.array(regional_data.iloc[:,train_period])\n",
    "        a_xi = a_xi.reshape(regional_data.shape[0],1)\n",
    "        \n",
    "    \n",
    "    regional_predictions = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_t_preds + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds  \n",
    "    regional_predictions_upconf = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_t_preds_upconf + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds_upconf \n",
    "    regional_predictions_lowconf = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_t_preds_lowconf + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds_lowconf\n",
    "    \n",
    "    \n",
    "    regional_preds = pd.DataFrame(regional_predictions)\n",
    "    regional_preds.index = country_data.index\n",
    "    regional_preds.columns = country_data.columns\n",
    "    errors = MAPE(regional_preds.iloc[:,train_period:],country_data.iloc[:,train_period:])\n",
    "    model_error = np.sum(errors)\n",
    "    \n",
    "    for i in range(regional_terms):\n",
    "        p_value = sm.stats.acorr_ljungbox(k_ts[i,:], lags=[len(k_ts[i,:]) - 1],return_df = True).iloc[0,1]\n",
    "        print(\"p_value k_t\" +str(i)+\" after re-estimation: \" +str(p_value)+ \" \\n\")\n",
    "        if p_value > 0.05:\n",
    "                print(\"Regional k_t\"+str(i)+ \" is white noise with LB p_value \"+ str(p_value)+\" \\n\")\n",
    "        \n",
    "\n",
    "    return a_xi,B_xs,K_ts,b_xs,k_ts,country_re_est,regional_re_est,K_t_models,k_t_models,K_t_preds,k_t_preds,regional_predictions,regional_predictions_upconf,regional_predictions_lowconf,model_error,res,D_country,D\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be140c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Percentage Error\n",
    "def MAPE(predictions,test_set):\n",
    "    errors = test_set.subtract(predictions)\n",
    "    results = []\n",
    "    for i in range(0,test_set.shape[0]): \n",
    "        results = np.append(results,np.mean(np.abs(errors.iloc[i,:].divide(test_set.iloc[i,:])))*100)\n",
    "    return results\n",
    "\n",
    "# symmetric Mean Absolute Percentage Error\n",
    "def sMAPE(predictions,test_set):\n",
    "    errors = test_set.subtract(predictions)\n",
    "    results = []\n",
    "    for i in range(0,test_set.shape[0]):\n",
    "        sum_pred_act = np.abs(test_set.iloc[i,:]) + np.abs(predictions.iloc[i,:])\n",
    "        err = 2*np.abs(errors.iloc[i,:])\n",
    "        results = np.append(results,np.mean(err.divide(sum_pred_act))*100)\n",
    "    return results\n",
    "    \n",
    "# Root Mean Squared Error\n",
    "def RMSE(predictions,test_set):\n",
    "    errors = test_set.subtract(predictions)\n",
    "    results = []\n",
    "    for i in range(0,test_set.shape[0]):\n",
    "        results = np.append(results,np.sqrt(np.mean(errors.iloc[i,:]**2)))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plots(prediction_matrix,\n",
    "          prediction_upconf_matrix,\n",
    "          prediction_lowconf_matrix,\n",
    "          test_period,\n",
    "          model_data,\n",
    "          suptitle = \"Title\",\n",
    "          n=6,\n",
    "          tick_rotation = 45,\n",
    "          figure_size = (13,20)\n",
    "         ):\n",
    "    plt.figure()\n",
    "    fig,ax = plt.subplots(model_data.shape[0],1,figsize = figure_size)\n",
    "    fig.supylabel(\"Log-Mortality Rate\")\n",
    "    \n",
    "    for i in range(model_data.shape[0]):\n",
    "        ax[i].plot(prediction_matrix.iloc[i,:],label = \"Estimates\")\n",
    "        ax[i].plot(model_data.loc[model_data.index[i]],label = \"True Series\")\n",
    "        ax[i].set_title(model_data.index[i])\n",
    "        ax[i].set_xticks(ax[i].get_xticks()[::n])\n",
    "        ax[i].tick_params(rotation = tick_rotation)\n",
    "        ax[i].axvspan(len(model_data.loc[model_data.index[0]]) - test_period, len(model_data.loc[model_data.index[0]]) , color='green', alpha=0.25)\n",
    "        ax[i].plot(prediction_upconf_matrix.iloc[i,-test_period:],color = \"red\",linestyle = \"dashed\",label = \"95% Confidence Interval\")\n",
    "        ax[i].plot(prediction_lowconf_matrix.iloc[i,-test_period:],color = \"red\",linestyle = \"dashed\")\n",
    "        ax[i].legend()\n",
    "        \n",
    "    fig.suptitle(suptitle, y=1.02)  # Adjust the y position of the suptitle\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.99) \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6dd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exog_variables(temp_factors,region,train_period = 60,test_period = 24):\n",
    "    av_temp = np.array(temp_factors.loc[temp_factors['name'] == region].loc[:,\"monthly_avgtemp\"])\n",
    "    av_temp_train = av_temp[:train_period]\n",
    "    av_temp_test = av_temp[train_period:train_period+test_period]\n",
    "\n",
    "    temp_below = np.array(temp_factors.loc[temp_factors['name'] == region].loc[:,\"days_below_7\"])\n",
    "    temp_below_train = temp_below[:train_period]\n",
    "    temp_below_test = temp_below[train_period:train_period+test_period]\n",
    "\n",
    "    av_humid = np.array(temp_factors.loc[temp_factors['name'] == region].loc[:,\"monthly_avghumidity\"])\n",
    "    av_humid_train = av_humid[:train_period]\n",
    "    av_humid_test = av_humid[train_period:train_period+test_period]\n",
    "    \n",
    "    temp_above = np.array(temp_factors.loc[temp_factors['name'] == region].loc[:,\"days_above_20\"])\n",
    "    temp_above_train = temp_above[:train_period]\n",
    "    temp_above_test = temp_above[train_period:train_period+test_period]\n",
    "\n",
    "    exog_train = np.zeros((len(av_temp_train),4))\n",
    "    exog_train[:,0] = av_temp_train\n",
    "    exog_train[:,1] = av_humid_train\n",
    "    exog_train[:,2] = temp_below_train\n",
    "    exog_train[:,3] = temp_above_train\n",
    "    exog_train = pd.DataFrame(exog_train)\n",
    "    exog_train.columns = [\"av_temp\",\"av_humid\",\"temp_below\",\"temp_above\"]\n",
    "\n",
    "    exog_test = np.zeros((len(av_temp_test),4))\n",
    "    exog_test[:,0] = av_temp_test\n",
    "    exog_test[:,1] = av_humid_test\n",
    "    exog_test[:,2] = temp_below_test\n",
    "    exog_test[:,3] = temp_above_test\n",
    "    exog_test = pd.DataFrame(exog_test)\n",
    "    exog_test.columns = [\"av_temp\",\"av_humid\",\"temp_below\",\"temp_above\"]\n",
    "    \n",
    "    return exog_train,exog_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f77d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### experimental\n",
    "\n",
    "def k_t_terms(country_data,regional_data, common_terms = 1,regional_terms = 1,prediction_period = 36,train_period = 84):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import pmdarima as pm\n",
    "    from functools import reduce\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "        \n",
    "    \n",
    "    regional_logmort = regional_data.iloc[:,:train_period]\n",
    "    country_logmort = country_data.iloc[:,:train_period]\n",
    "    regional_test = regional_data.iloc[:,train_period:]\n",
    "    country_test = country_data.iloc[:,train_period:]\n",
    "        \n",
    "\n",
    "    if country_data.shape != regional_data.shape:\n",
    "        return \" Error Country and Regional table shapes do not match\"\n",
    "\n",
    "     ### LC for country level \n",
    "    a_x = country_logmort.mean(axis = 1) # average over time for each age group\n",
    "    a_x = np.zeros(a_x.shape)\n",
    "    A_xt = country_logmort.sub(a_x,axis = 0) #logmort - a_x\n",
    "    a_x = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D_country,V_t = np.linalg.svd(A_xt,full_matrices = False) # SVD of A_xt (should it be only logmort as in Lee-Li model?)\n",
    "\n",
    "    B_xs = np.zeros((country_logmort.shape[0],common_terms)) # columns are B_x terms\n",
    "    K_ts = np.zeros((country_logmort.shape[1],common_terms)) # rows are K_t terms \n",
    "    country_re_est = np.zeros(country_logmort.shape)\n",
    "    for i in range(common_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        B_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(len(k_t),))\n",
    "        K_ts[:,i] = k_t\n",
    "        #country_re_est = country_re_est + b_x@k_t\n",
    "            \n",
    "        \n",
    "\n",
    "        ### LC for regional level\n",
    "    regional_logmort_adj = regional_logmort - country_re_est\n",
    "    a_xi = regional_logmort_adj.mean(axis = 1) # average over time for each age group\n",
    "    A_xt_reg = regional_logmort_adj.sub(a_xi,axis = 0) #logmort - a_x\n",
    "    a_xi = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D,V_t = np.linalg.svd(A_xt_reg,full_matrices = False) # SVD of A_xt\n",
    "\n",
    "    b_xs = np.zeros((regional_logmort.shape[0],regional_terms)) # columns are b_x terms\n",
    "    k_ts = np.zeros((regional_logmort.shape[1],regional_terms)) # rows are k_t terms\n",
    "    regional_re_est = np.zeros(regional_logmort.shape)\n",
    "    for i in range(regional_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        b_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(len(k_t),))\n",
    "        k_ts[:,i] = k_t\n",
    "            \n",
    "        #regional_re_est = regional_re_est + b_x@k_t\n",
    "    #regional_re_est = regional_re_est + a_xi\n",
    "\n",
    "    all_k_t = pd.DataFrame(np.hstack((K_ts,k_ts)))\n",
    "    \n",
    "\n",
    "    return all_k_t, K_ts,k_ts,B_xs,b_xs,a_xi\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37aae84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### experimental\n",
    "def reverse_differencing(diff_series,initial_value):\n",
    "    diff_inv = [initial_value]\n",
    "\n",
    "    for i in range(len(diff_series)-2):\n",
    "        diff_inv = np.append(diff_inv,diff_inv[i] + diff_series.iloc[i+2])\n",
    "\n",
    "    diff_inv = pd.DataFrame(diff_inv)\n",
    "    return diff_inv\n",
    "\n",
    "def VARMAX_model(country_data,regional_data,\n",
    "                 common_terms = 1,regional_terms = 1,\n",
    "                 prediction_period = 36,train_period = 84,\n",
    "                 AR = 2,MA = 2,iters = 100,\n",
    "                 exogenous_train = None,exogenous_test = None,\n",
    "                country_deaths = None, country_pop = None,\n",
    "                region_deaths = None, region_pop = None,\n",
    "                 re_est = False,smooth = False):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import pmdarima as pm\n",
    "    from functools import reduce\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    if smooth:\n",
    "        regional_data = ma_smoothing(regional_data)\n",
    "        country_data = ma_smoothing(country_data)\n",
    "        \n",
    "    \n",
    "    regional_logmort = regional_data.iloc[:,:train_period]\n",
    "    country_logmort = country_data.iloc[:,:train_period]\n",
    "    regional_test = regional_data.iloc[:,train_period:]\n",
    "    country_test = country_data.iloc[:,train_period:]\n",
    "        \n",
    "\n",
    "    if country_data.shape != regional_data.shape:\n",
    "        return \" Error Country and Regional table shapes do not match\"\n",
    "\n",
    "     ### LC for country level \n",
    "    a_x = country_logmort.mean(axis = 1) # average over time for each age group\n",
    "    A_xt = country_logmort.sub(a_x,axis = 0) #logmort - a_x\n",
    "    a_x = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D_country,V_t = np.linalg.svd(A_xt,full_matrices = False) # SVD of A_xt (should it be only logmort as in Lee-Li model?)\n",
    "\n",
    "    B_xs = np.zeros((country_logmort.shape[0],common_terms)) # columns are B_x terms\n",
    "    K_ts = np.zeros((country_logmort.shape[1],common_terms)) # rows are K_t terms \n",
    "    country_re_est = np.zeros(country_logmort.shape)\n",
    "    for i in range(common_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        B_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(len(k_t),))\n",
    "        K_ts[:,i] = k_t\n",
    "        #country_re_est = country_re_est + b_x@k_t\n",
    "            \n",
    "        \n",
    "\n",
    "        ### LC for regional level\n",
    "    regional_logmort_adj = regional_logmort - country_re_est\n",
    "    a_xi = regional_logmort_adj.mean(axis = 1) # average over time for each age group\n",
    "    A_xt_reg = regional_logmort_adj.sub(a_xi,axis = 0) #logmort - a_x\n",
    "    a_xi = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D,V_t = np.linalg.svd(A_xt_reg,full_matrices = False) # SVD of A_xt\n",
    "\n",
    "    b_xs = np.zeros((regional_logmort.shape[0],regional_terms)) # columns are b_x terms\n",
    "    k_ts = np.zeros((regional_logmort.shape[1],regional_terms)) # rows are k_t terms\n",
    "    regional_re_est = np.zeros(regional_logmort.shape)\n",
    "    for i in range(regional_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        b_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(len(k_t),))\n",
    "        k_ts[:,i] = k_t\n",
    "            \n",
    "        #regional_re_est = regional_re_est + b_x@k_t\n",
    "    #regional_re_est = regional_re_est + a_xi\n",
    "    if re_est:\n",
    "        K_ts_reest = K_ts.transpose()\n",
    "        #print(K_ts_reest.shape)\n",
    "        K_ts_reest = K_ts.flatten() #flatten as minimize only takes 1d array of parameters\n",
    "        #print(objective(K_ts_reest,np.zeros((country_pop.shape[0],1)), B_xs,D_country, country_pop, country_deaths,common_terms))\n",
    "        result = minimize(objective, K_ts_reest, args=(a_x, B_xs,D_country, country_pop, country_deaths,common_terms), method='BFGS') #np.zeros((country_pop.shape[0],1))\n",
    "        #print(objective(result.x,np.zeros((country_pop.shape[0],1)), B_xs,D_country, country_pop, country_deaths,common_terms))\n",
    "        K_ts = result.x.reshape(((common_terms,country_pop.shape[1])))\n",
    "        K_ts = K_ts.transpose()\n",
    "        \n",
    "        k_ts_reest = k_ts.transpose()\n",
    "        k_ts_reest = k_ts.flatten()\n",
    "#         print(\"k_ts \" +str(k_ts_reest.shape))\n",
    "#         print(\"b_xs \" + str(b_xs.shape))\n",
    "#         print(\"K_ts \" + str(K_ts.shape))\n",
    "#         print(\"B_xs \" + str(B_xs.shape))\n",
    "#         print(\"a_xi \" + str(a_xi.shape))\n",
    "#         print(\"region_pop \" + str(region_pop.shape))\n",
    "#         print(\"region_deaths \" + str(region_deaths.shape))\n",
    "#         print(\"Objective before minimsation \" + str(regional_objective(k_ts_reest,b_xs,K_ts.transpose(),B_xs,a_xi,region_pop,region_deaths,regional_terms)))\n",
    "        result = minimize(regional_objective,k_ts_reest,args = (b_xs,K_ts.transpose(),B_xs,a_xi,D_country,D,region_pop,region_deaths,regional_terms),method = 'BFGS')\n",
    "        #print(\"Objective before minimsation \" + str(regional_objective(result.x,b_xs,K_ts.transpose(),B_xs,a_xi,region_pop,region_deaths,regional_terms)))\n",
    "        k_ts = result.x.reshape(((regional_terms,region_pop.shape[1])))\n",
    "        k_ts = k_ts.transpose()\n",
    "    \n",
    "        \n",
    "    all_k_t = pd.DataFrame(np.hstack((K_ts,k_ts)))\n",
    "    original = all_k_t\n",
    "                                                                  \n",
    "                                                        \n",
    "    # dealing with non-stationarity                                                       \n",
    "    differences = [] # number differences done for each time-series/column\n",
    "    non_stat_cols = [] # returns index of non-stationary columns \n",
    "    for i in range(all_k_t.shape[1]):\n",
    "        series = all_k_t.iloc[:,i]\n",
    "        p_value = adfuller(series)[1]\n",
    "        diff = 0\n",
    "        if p_value > 0.05:\n",
    "            non_stat_cols = np.append(non_stat_cols,i)\n",
    "            #print(\"Non-Stationary k term : \" + str(i))\n",
    "            \n",
    "        while p_value > 0.05:\n",
    "            series = series.diff()\n",
    "            series.iloc[0] = series.iloc[1]\n",
    "            p_value = adfuller(series)[1]\n",
    "            diff = diff + 1\n",
    "            #print(diff)\n",
    "            #print(p_value)\n",
    "        differences = np.append(differences,diff)\n",
    "        all_k_t.iloc[:,i] = series\n",
    "        #print(differences)\n",
    "        #print(\"Non_stat_cols\" + str(non_stat_cols))\n",
    "    \n",
    "        \n",
    "    \n",
    "    model = VARMAX(all_k_t, order = (AR,MA),exog = exogenous_train,enforce_stationarity = False ,initialization = \"approximate_diffuse\")\n",
    "    results = model.fit(maxiter = iters,disp = False)\n",
    "    preds = results.get_forecast(steps = prediction_period,exog = exogenous_test)\n",
    "    all_k_lowconf = preds.conf_int().iloc[:,:common_terms+regional_terms]\n",
    "    all_k_upconf = preds.conf_int().iloc[:,common_terms+regional_terms:]\n",
    "    \n",
    "    all_k_preds = preds.predicted_mean\n",
    "    all_k_preds = pd.concat([all_k_t,all_k_preds])\n",
    "\n",
    "    all_k_lowconf.columns = all_k_preds.columns\n",
    "    all_k_upconf.columns = all_k_preds.columns\n",
    "    all_k_preds_lowconf = pd.concat([all_k_t,all_k_lowconf])\n",
    "    all_k_preds_upconf = pd.concat([all_k_t,all_k_upconf])\n",
    "    \n",
    "    # reversing any differencing done \n",
    "    for i in non_stat_cols:\n",
    "        pred_rev = reverse_differencing(all_k_preds.iloc[:,int(i)],original.iloc[0,int(i)])\n",
    "        #pred_rev = pred_rev[1:]\n",
    "        all_k_preds.iloc[:,int(i)] = pred_rev\n",
    "        \n",
    "        pred_lowconf_rev = reverse_differencing(all_k_preds_lowconf.iloc[:,int(i)],original.iloc[0,int(i)])\n",
    "        #pred_lowconf_rev = pred_lowconf_rev[1:]\n",
    "        all_k_preds_lowconf.iloc[:,int(i)] = pred_lowconf_rev\n",
    "        \n",
    "        pred_upconf_rev = reverse_differencing(all_k_preds_upconf.iloc[:,int(i)],[original.iloc[0,int(i)]])\n",
    "        #pred_upconf_rev = pred_upconf_rev[1:]\n",
    "        all_k_preds_upconf.iloc[:,int(i)] = pred_upconf_rev\n",
    "        \n",
    "    predictions = pd.DataFrame(B_xs@np.diag(D_country)[:common_terms,:common_terms]@np.array(all_k_preds.iloc[:,0:common_terms].transpose()) + b_xs@np.diag(D)[:regional_terms,:regional_terms]@np.array(all_k_preds.iloc[:,common_terms:].transpose()) + a_xi)\n",
    "    predictions_lowconf = pd.DataFrame(B_xs@np.diag(D_country)[:common_terms,:common_terms]@np.array(all_k_preds_lowconf.iloc[:,0:common_terms].transpose()) + b_xs@np.diag(D)[:regional_terms,:regional_terms]@np.array(all_k_preds_lowconf.iloc[:,common_terms:].transpose()) + a_xi)\n",
    "    predictions_upconf = pd.DataFrame(B_xs@np.diag(D_country)[:common_terms,:common_terms]@np.array(all_k_preds_upconf.iloc[:,0:common_terms].transpose()) + b_xs@np.diag(D)[:regional_terms,:regional_terms]@np.array(all_k_preds_upconf.iloc[:,common_terms:].transpose()) + a_xi)\n",
    "    \n",
    "    preds = pd.DataFrame(predictions)\n",
    "    test_preds = preds.iloc[:,train_period:]\n",
    "    test_preds.index = regional_test.index\n",
    "    test_preds.columns = regional_test.columns\n",
    "    \n",
    "\n",
    "    test_errors = MAPE(test_preds,regional_test)\n",
    "\n",
    "    aic = results.aic\n",
    "    return all_k_t, K_ts,k_ts,B_xs,b_xs,a_xi,all_k_preds,predictions,predictions_lowconf,predictions_upconf,results,aic,test_errors,D_country,D\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de22cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LC_model(logmort_table,no_terms = 1,re_est = False,pop = None,death = None,prediction_period = 36):\n",
    "    \n",
    "    a_x = logmort_table.mean(axis = 1) # average over time for each age group\n",
    "    A_xt = logmort_table.sub(a_x,axis = 0) #logmort - a_x\n",
    "    a_x = np.reshape(a_x,(len(a_x),1))\n",
    "\n",
    "    U,D,V_t = np.linalg.svd(A_xt,full_matrices = False) # SVD of A_xt (should it be only logmort as in Lee-Li model?)\n",
    "   \n",
    "    b_xs = np.zeros((logmort_table.shape[0],no_terms)) \n",
    "    k_ts = np.zeros((no_terms,logmort_table.shape[1]))  \n",
    "    \n",
    "    for i in range(no_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        #b_x = b_x/(np.sum(b_x))\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        b_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(len(k_t),))\n",
    "        k_ts[i,:] = k_t\n",
    "        \n",
    "    if re_est:\n",
    "        \n",
    "        k_ts_reest = k_ts.flatten()\n",
    "        print(\"Objecive before minimisation \" +str(objective(k_ts_reest,a_xi,b_xs,D,pop,death,no_terms)))\n",
    "        result = minimize(objective,k_ts_reest,args = (a_xi,b_xs,D,pop,death,no_terms),method = 'BFGS')\n",
    "        print(\"Objective after minimisation \" + str(objective(result.x,a_xi,b_xs,D,pop,death,no_terms)))\n",
    "        k_ts = result.x.reshape(((no_terms,pop.shape[1])))\n",
    "        \n",
    "    \n",
    "    re_est = pd.DataFrame(a_x + b_xs@np.diag(D)[:no_terms,:no_terms]@k_ts,columns = logmort_table.columns,index = logmort_table.index)\n",
    "\n",
    "    return re_est,a_x,b_xs,k_ts,D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d68b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distribute_pop(df):\n",
    "    # Initialize the output DataFrame with zeros and the same index and columns as the input df\n",
    "    output_df = pd.DataFrame(np.zeros(df.shape), index=df.index, columns=df.columns)\n",
    "\n",
    "    # Populate the output DataFrame\n",
    "    for row in range(df.shape[0]):\n",
    "        for i in range(df.shape[1]):\n",
    "            if i % 12 == 0:\n",
    "                output_df.iloc[row, i] = df.iloc[row, i]\n",
    "            else:\n",
    "                # Calculate the incremental difference only if next_base_col is within bounds\n",
    "                base_col = (i // 12) * 12\n",
    "                next_base_col = base_col + 12\n",
    "                if next_base_col < df.shape[1]:\n",
    "                    increment = (df.iloc[row, next_base_col] - df.iloc[row, base_col]) / 12\n",
    "                    # Add the appropriate multiple of the increment to the base column value\n",
    "                    output_df.iloc[row, i] = df.iloc[row, base_col] + increment * (i % 12)\n",
    "                else:\n",
    "                    # If next_base_col is out of bounds, use the last valid increment\n",
    "                    increment = (df.iloc[row, -1] - df.iloc[row, base_col]) / (df.shape[1] - base_col)\n",
    "                    output_df.iloc[row, i] = df.iloc[row, base_col] + increment * (i % 12)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_no_common(regional_data,regional_terms = 1,\n",
    "               prediction_period = 36,train_period = 84,\n",
    "               exogenous_train = None,exogenous_test = None,\n",
    "               region_deaths = None, region_pop = None,\n",
    "               re_est = False,smooth = False):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import pmdarima as pm\n",
    "    from functools import reduce\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # applies a moving average smoothing to the data \n",
    "    if smooth:\n",
    "        regional_data = ma_smoothing(regional_data)\n",
    "    \n",
    "    # create training and testing sets\n",
    "    regional_logmort = regional_data.iloc[:,:train_period]    \n",
    "    regional_test = regional_data.iloc[:,train_period:]\n",
    "        \n",
    "                  \n",
    "        ### LC for regional level\n",
    "    regional_logmort_adj = regional_logmort\n",
    "    a_xi = regional_logmort_adj.mean(axis = 1) # average over time for each age group\n",
    "    a_xi = np.reshape(a_xi,(len(a_xi),1))\n",
    "    A_xt_reg = regional_logmort_adj.sub(a_xi,axis = 1) #logmort - a_x ,SVD applied to this (centering of matrix) \n",
    "    \n",
    "    U,D,V_t = np.linalg.svd(A_xt_reg,full_matrices = False) # SVD of A_xt\n",
    "\n",
    "    b_xs = np.zeros((regional_logmort.shape[0],regional_terms)) # columns are b_x terms\n",
    "    k_ts = np.zeros((regional_terms,regional_logmort.shape[1])) # rows are k_t terms (right singular vectors)\n",
    "    #regional_re_est = np.zeros(regional_logmort.shape)\n",
    "    for i in range(regional_terms):\n",
    "        b_x = U[:,i]\n",
    "        b_x = b_x/(np.sqrt(np.sum(b_x**2))) # adjusting so that b_x^2 sums to 1 (works better why?)\n",
    "        b_x = np.reshape(b_x,(len(b_x),))\n",
    "        b_xs[:,i] = b_x\n",
    "        b_x = np.reshape(b_x,(len(b_x),1))\n",
    "\n",
    "        k_t = V_t[i,:]\n",
    "        p_value = sm.stats.acorr_ljungbox(k_t, lags=[len(k_t) - 1],return_df = True).iloc[0,1]\n",
    "        print(\"p_value k_t\" +str(i)+\" before re-estimation: \" +str(p_value)+\" \\n\")\n",
    "        if p_value > 0.05:\n",
    "            print(\"Regional k_t\"+str(i)+ \" is white noise with LB p_value \"+ str(p_value)+ \" \\n\")\n",
    "        k_t = k_t - np.mean(k_t) # adjusting so that k_t sums to 0\n",
    "        k_t = np.reshape(k_t,(1,len(k_t)))\n",
    "        k_ts[i,:] = k_t\n",
    "            \n",
    "        #regional_re_est = regional_re_est + b_x@k_t\n",
    "   \n",
    "    # re-estimation of k_t terms\n",
    "    res = 0\n",
    "    if re_est:\n",
    "        \n",
    "        k_ts_reest = k_ts.flatten()\n",
    "        print(\"Objecive before minimisation \" +str(objective(k_ts_reest,a_xi, b_xs,D, region_pop, region_deaths,regional_terms)))\n",
    "        result = minimize(objective, k_ts_reest, args=(a_xi, b_xs,D, region_pop, region_deaths,regional_terms), method='BFGS')\n",
    "        print(\"Objective after minimisation \" + str(objective(result.x,a_xi, b_xs,D, region_pop, region_deaths,regional_terms)))\n",
    "        k_ts = result.x.reshape(((regional_terms,region_pop.shape[1])))\n",
    "        \n",
    "    #regional_re_est = regional_re_est + a_xi\n",
    "    regional_re_est = a_xi + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts \n",
    "    #a_xi_adjustment = np.subtract(np.array(regional_logmort),regional_re_est)\n",
    "    #a_xi = a_xi + np.mean(a_xi_adjustment,axis = 1).transpose()\n",
    "    #regional_re_est = a_xi + B_xs@np.diag(D_country)[:common_terms,:common_terms]@K_ts + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts\n",
    "    a_xi = np.reshape(np.mean(np.subtract(np.array(regional_logmort),regional_re_est),axis = 1),(regional_logmort.shape[0],1)) +a_xi\n",
    "    regional_re_est = a_xi  + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_ts\n",
    "\n",
    "\n",
    "        # models for regional terms \n",
    "    k_t_models = {}\n",
    "    k_t_preds = np.zeros((regional_terms,train_period + prediction_period))\n",
    "    k_t_preds_upconf = np.zeros((regional_terms,train_period + prediction_period))\n",
    "    k_t_preds_lowconf = np.zeros((regional_terms,train_period + prediction_period)) #regional_logmort.shape[0]\n",
    "    for i in range(regional_terms):\n",
    "        k_t_model = pm.auto_arima(k_ts[i,:].flatten(),\n",
    "                               exogenous_train,\n",
    "                               start_p = 0,\n",
    "                               start_q = 0,\n",
    "                               max_p = 15,\n",
    "                               max_q = 15,\n",
    "                               max_d = 4,\n",
    "                               m = 12,\n",
    "                               seasonal = True,\n",
    "                               suppress_warnings=True,\n",
    "                               error_action='ignore',with_intercept = True)\n",
    "\n",
    "        k_t_models[f'model_{i}'] = k_t_model\n",
    "        k_t_model_preds,k_t_model_confint = k_t_model.predict(n_periods = prediction_period,X = exogenous_test,return_conf_int = True)\n",
    "        k_t_preds[i,:] = np.append(k_ts[i,:],np.array(k_t_model_preds).reshape((1,len(k_t_model_preds))))\n",
    "        k_t_preds_upconf[i,:] = np.append(k_ts[i,:],np.array(k_t_model_confint[:,0]).reshape((1,len(k_t_model_confint[:,0]))))\n",
    "        k_t_preds_lowconf[i,:] = np.append(k_ts[i,:],np.array(k_t_model_confint[:,1]).reshape((1,len(k_t_model_confint[:,1]))))\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    regional_predictions = a_xi  + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds  \n",
    "    regional_predictions_upconf = a_xi +  b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds_upconf \n",
    "    regional_predictions_lowconf = a_xi + b_xs@np.diag(D)[:regional_terms,:regional_terms]@k_t_preds_lowconf\n",
    "    \n",
    "    \n",
    "    regional_preds = pd.DataFrame(regional_predictions)\n",
    "    regional_preds.index = regional_data.index\n",
    "    regional_preds.columns = regional_data.columns\n",
    "    errors = MAPE(regional_preds.iloc[:,train_period:],regional_data.iloc[:,train_period:])\n",
    "    model_error = np.sum(errors)\n",
    "    \n",
    "    prop_k_t_pos = k_ts\n",
    "    prop_k_t_pos[prop_k_t_pos >0] = 1\n",
    "    prop_k_t_pos[prop_k_t_pos <0] = 0\n",
    "    prop_k_t_pos = np.sum(prop_k_t_pos,axis = 1)/prop_k_t_pos.shape[1]\n",
    "    print(\"Proportion of k_t values which are positive\" + str(prop_k_t_pos) + \" \\n\")\n",
    "    \n",
    "    prop_b_x_pos = b_xs\n",
    "    prop_b_x_pos[prop_b_x_pos > 0] = 1\n",
    "    prop_b_x_pos[prop_b_x_pos < 0] = 0\n",
    "    prop_b_x_pos = np.sum(prop_b_x_pos,axis = 0)/prop_b_x_pos.shape[0]\n",
    "    print(\"Proportion of b_x values which are positive\" + str(prop_b_x_pos) + \" \\n\")\n",
    "    \n",
    "    for i in range(regional_terms):\n",
    "        p_value = sm.stats.acorr_ljungbox(k_ts[i,:], lags=[len(k_ts[i,:]) - 1],return_df = True).iloc[0,1]\n",
    "        print(\"p_value k_t\" +str(i)+\" after re-estimation: \" +str(p_value)+ \" \\n\")\n",
    "        if p_value > 0.05:\n",
    "                print(\"Regional k_t\"+str(i)+ \" is white noise with LB p_value \"+ str(p_value)+\" \\n\")\n",
    "        \n",
    "\n",
    "    return a_xi,b_xs,k_ts,regional_re_est,k_t_models,k_t_preds,regional_predictions,regional_predictions_upconf,regional_predictions_lowconf,model_error,res,D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03c90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for splitting data frame into epidemic years, \n",
    "\n",
    "\n",
    "def epi_year(df):\n",
    "    epi_years = {}\n",
    "    length = df.shape[1]\n",
    "    years = int(length/12)\n",
    "    for i in range(years):\n",
    "        epi_years[f\"year_{i}\"] = df.iloc[:,(i*12 + 6):((i+1)*12 + 6)]\n",
    "    return epi_years\n",
    "\n",
    "\n",
    "def excess_sum(death_table_df):\n",
    "        epi_years = epi_year(death_table_df)\n",
    "        epi_year_avgs = {}\n",
    "        print(\"Note: year \"+ str(len(epi_years)-1) + \" is only half an epiyear!  + First 6 months disregarded\")\n",
    "        for i in range(len(epi_years)):\n",
    "            epi_year_avgs[f\"year_{i}\"] = epi_years[f\"year_{i}\"].sum(axis = 1)\n",
    "            \n",
    "        return epi_year_avgs\n",
    "    \n",
    "    \n",
    "def epi_year_lin_ests(death_table_df):\n",
    "    excess_avg = excess_sum(death_table_df)\n",
    "\n",
    "    expected_death = np.zeros((death_table_df.shape[0],1))\n",
    "    for i in range(len(excess_avg)-1):\n",
    "        expected_death = expected_death + np.reshape(excess_avg[f\"year_{i}\"],expected_death.shape)\n",
    "\n",
    "    expected_death = expected_death/(len(excess_avg)-1)\n",
    "    return expected_death\n",
    "\n",
    "\n",
    "#Averaging estimate for second half of epidemic year \n",
    "def est_half2_epiyear(death_table):\n",
    "    death_count = {}\n",
    "    expected_death = np.zeros((death_table.shape[0],))\n",
    "    for i in range(len(epi_year(pd.DataFrame(death_table)).keys()) -1):\n",
    "        death_count[f\"year_{i}\"] = epi_year(pd.DataFrame(death_table))[list(epi_year(pd.DataFrame(death_table)).keys())[i]].iloc[:,6:].sum(axis = 1)\n",
    "        expected_death = expected_death + death_count[f\"year_{i}\"]\n",
    "\n",
    "    expected_death = expected_death/len(death_count.keys())\n",
    "    return expected_death\n",
    "\n",
    "#Averaging estimate for first half of epidemic year\n",
    "def est_half1_epiyear(death_table):\n",
    "    death_count = {}\n",
    "    expected_death = np.zeros((death_table.shape[0],))\n",
    "    for i in range(len(epi_year(pd.DataFrame(death_table)).keys()) -1):\n",
    "        death_count[f\"year_{i}\"] = epi_year(pd.DataFrame(death_table))[list(epi_year(pd.DataFrame(death_table)).keys())[i]].iloc[:,:6].sum(axis = 1)\n",
    "        expected_death = expected_death + death_count[f\"year_{i}\"]\n",
    "\n",
    "    expected_death = expected_death/len(death_count.keys())\n",
    "    return expected_death\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ebecee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete: 28/03/2024 13:38:24\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Run Complete: \"+ dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b226a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
